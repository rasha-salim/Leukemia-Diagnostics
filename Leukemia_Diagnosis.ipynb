{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Leukemia_Diagnosis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamesMum/Leukemia-Diagnostics/blob/master/Leukemia_Diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msp1ts7GAaz3",
        "colab_type": "text"
      },
      "source": [
        "# Leukemia Diagnostic Model\n",
        "\n",
        "**Classification of Acute Leukemia using Pretrained Deep Convolutional Neural Networks**\n",
        "Based on the implementation in the paper:\n",
        "\n",
        "[**Human-level recognition of blast cells in acute myeloid\n",
        "leukemia with convolutional neural networks**](https://www.biorxiv.org/content/10.1101/564039v1.full.pdf)\n",
        "\n",
        " **The Dataset used in this implementation:**\n",
        "\n",
        "\n",
        "- AML dataset\n",
        "-The number of classes are 10 \n",
        "\n",
        "- link to the dataset https://www.kaggle.com/lsaa2014/single-cell-morphological-dataset-of-leukocytes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "l6xo5rcqAaz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inporting the necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets, models\n",
        "import math\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision as tv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sXDOM-EcAa0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print(\"CUDA is available. Training on GPU!\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Training on CPU.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S3sz5Ao9Aa0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time to prepare the data\n",
        "\n",
        "batch_size = 32\n",
        "test_size = 0.20\n",
        "valid_size = 0.20\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "#define the transforms\n",
        "train_transform  = transforms.Compose([\n",
        "                                        transforms.Resize((400,400)),\n",
        "                                        transforms.RandomRotation(359),\n",
        "                                        transforms.RandomHorizontalFlip(0.2),\n",
        "                                        transforms.RandomVerticalFlip(0.2),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "\n",
        "\n",
        "train_data = datasets.ImageFolder(\"/kaggle/input/single-cell-morphological-dataset-of-leukocytes/blood_smear_images_for_aml_diagnosis_MOD/AML-Cytomorphology_LMU_MOD\",\n",
        "                                  transform = train_transform)\n",
        "\n",
        "#obtain training indicies that will be used as testing and validation\n",
        "\n",
        "num_train = len(train_data)\n",
        "indicies = list(range(num_train))\n",
        "np.random.shuffle(indicies)\n",
        "test_split = int(np.floor(test_size * num_train))\n",
        "valid_split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "train_idx, valid_idx, test_idx = indicies[test_split+valid_split:], indicies[:valid_split], indicies[valid_split:test_split+valid_split]\n",
        "\n",
        "#define samplers for obtainig the trainig, testing and validation set\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                                           sampler = train_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                                          sampler = test_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,\n",
        "                                          sampler = valid_sampler)\n",
        "\n",
        "\n",
        "#  PROMYELOCYTE (PMB Promyelocyte (bilobled))\n",
        "# PMO Promyelocyte), MYELOCYTE (MYB Myelocyte, MYO Myeloblast)ARE FOUND ON LEUKEMIA PATIENTS \n",
        "classes = ['BAS', 'EBO', 'EOS', 'KSC','LYT','MON', 'MYO', 'NGB', 'NGS', 'PMO']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GGNW_ve2Aa0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "def imshow(img):\n",
        "  img = img /2+0.5 #unormalize the images\n",
        "  plt.imshow(np.transpose(img, (1, 2, 0))) #convert it back from tensor to image\n",
        "\n",
        "#get one batch of training images\n",
        "dataiter = iter(train_loader) #now contains the first batch\n",
        "images, labels = dataiter.next() #images=the first batch of images, labels= the first batch of labels\n",
        "images = images.numpy() #convert the images to display them\n",
        "\n",
        "#plot the imahes in the batch along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[]) #(rows, cols, index, .., ..)\n",
        "  imshow(images[idx])\n",
        "  ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CiQTG0QaAa0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load AlexNet pretrained model\n",
        "model = models.vgg13(pretrained=True)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k8P4cW5yAa0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#freeze the model calssifier\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(25088, 25088)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout', nn.Dropout(0.5)),\n",
        "                          ('fc2', nn.Linear(25088, 2048)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout', nn.Dropout(0.5)),\n",
        "                          ('fc3', nn.Linear(2048, 1024)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('dropout', nn.Dropout(0.5)),\n",
        "                          ('fc4', nn.Linear(1024, 10))                                       \n",
        "                                      ]))\n",
        "#('output', nn.LogSoftmax(dim=1)\n",
        "\n",
        "model.classifier = classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV1cJWpKAa0X",
        "colab_type": "text"
      },
      "source": [
        "<h2>Choose how to train</h2>\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"#normal\">Normal Training</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#cyclical\">With Cyclical Learning Rate</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzpq4CUTAa0Y",
        "colab_type": "text"
      },
      "source": [
        "## Normal Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YsgHNTAVAa0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "#Loss function and optmixation function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.00005, momentum=0.9)\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i6NWcnCmAa0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "import numpy as np\n",
        "n_epochs = 60\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss (comapre the values of the output model to the actual labels)\n",
        "        loss = criterion(output, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for images, labels in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*images.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        valid_loss_min = valid_loss\n",
        "        # print the decremnet in the validation\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, valid_loss))\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min, \n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_AML_classifier.pt')\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, train_loss, valid_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GDXpiseiAa0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model_AML_classifier.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c28TmuD4Aa0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize the test loss\n",
        "test_loss = 0.0\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list (0. for i in range(10))\n",
        "\n",
        "#set the model to test and validation mode (no gradient descent needed)\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "  #move the tensor to GPU ig available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  #forward pass: compute prediction output by passing the first batch of test data\n",
        "  output = model(data)\n",
        "  #calculate the batch size\n",
        "  loss = criterion(output, target)\n",
        "  #update the test loss\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  #convert output probabilities to output class\n",
        "  _, pred = torch.max(output, 1)\n",
        "  #compare the prediction to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  #conveert to numpy array and remove the extra dimention and get only the result\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "  #calculate test accuracy for each object class\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      label = target.data[i] #get the corresponding label from the object\n",
        "      class_correct[label] += correct[i].item()\n",
        "      class_total[label] += 1\n",
        "    except IndexError:\n",
        "      break\n",
        "  \n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "     print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "       print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wbmlnoflAa0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the experiment\n",
        "lr_find_loss = []\n",
        "lr_find_lr = []\n",
        "\n",
        "iter = 0\n",
        "\n",
        "smoothing = 0.05\n",
        "for i in range(lr_find_epochs):\n",
        "  print(\"epoch {}\".format(i))\n",
        "  model.train()\n",
        "  for inputs, labels in train_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Get outputs to calc loss\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update LR\n",
        "    scheduler.step()\n",
        "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "    lr_find_lr.append(lr_step)\n",
        "\n",
        "    # smooth the loss\n",
        "    if iter==0:\n",
        "      lr_find_loss.append(loss)\n",
        "    else:\n",
        "      loss = smoothing  * loss + (1 - smoothing) * lr_find_loss[-1]\n",
        "      lr_find_loss.append(loss)\n",
        "     \n",
        "    iter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyiCO3yhAa0u",
        "colab_type": "text"
      },
      "source": [
        "## Implementing cyclic learning rate techneque:\n",
        "\n",
        "- First we need to find the max_lr and the base_lr.\n",
        "- implement the following experience to do so\n",
        "Refernce:\n",
        "- https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
        "- https://arxiv.org/pdf/1506.01186.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rRaYPourAa0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment parameters\n",
        "lr_find_epochs = 2\n",
        "start_lr = 1e-7\n",
        "end_lr = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cUA4BCH-Aa0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the model, optimizer and loss function for the experiment\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=start_lr)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# y = a.e(-bt) \n",
        "# end_lr = start_lr . e(b.t)\n",
        "# (end_lr - start_lr) = e(b.t)\n",
        "# ln(end_lr - start_lr) = b.t\n",
        "# b = ln(end_lr - start_lr) / t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DT1UCw7JAa02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LR function lambda\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len( train_loader)))\n",
        "scheduler = LambdaLR(optimizer, lr_lambda)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1DgYoJQAa07",
        "colab_type": "text"
      },
      "source": [
        "- In the following we run two epochs through the network. At each step we are capturing the LR and optimizing the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VU2-HHq1Aa08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move model to GPU\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8z3h81tVAa0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing a sample tested of data\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "#Move model inputs to cuda\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "#get sample outputs\n",
        "output = model(images)\n",
        "#convert probabilties to prediction class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images.cpu()[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X3YNeUsrAa1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"learning rate\")\n",
        "plt.xscale(\"log\")\n",
        "plt.plot(lr_find_lr, lr_find_loss)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V59DI6uGAa1F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- From the figure above, For the upper bound (max). We won't pick the one on lowest point but rather about a factor of ten to the left (0.5 is the lowest point). \n",
        "In this case\n",
        "the lowest point is about 3e-2 so we'll take 3e-2\n",
        "- Now for the lowe bound (Min): Acording to the paper and other resouce a good lower bound is the upper divided by 6. So 3e-3/6 = 5e-4\n",
        "\n",
        "- This approach could also help us find the range of acceptable lr for our model even if we decided to go with a fixed lr. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P3HL8D-GAa1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As concluded above\n",
        "lr_max = 3e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKeC77JxAa1J",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: The CLR Scheduale\n",
        "Which varies the learning rate between uper and lower bound.\n",
        "we are going with triangular CLR schedule.\n",
        "![triangular schedule](https://drive.google.com/file/d/1K6GraTrK4oV7GOokhFewG-xOmHqKbj75/view?usp=sharing)\n",
        "- Programatically we just need to create a custom function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "68bIh50wAa1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
        "\n",
        "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
        "    scaler = lambda x: 1.\n",
        "\n",
        "    # Lambda function to calculate the LR\n",
        "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
        "\n",
        "    # Additional function to see where on the cycle we are\n",
        "    def relative(it, stepsize):\n",
        "        cycle = math.floor(1 + it / (2 * stepsize))\n",
        "        x = abs(it / stepsize - 2 * cycle + 1)\n",
        "        return max(0, (1 - x)) * scaler(cycle)\n",
        "\n",
        "    return lr_lambda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auuZzoq6Aa1M",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Wrap it up\n",
        "- This can be wrapped up inside LamdaLR object in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zVlaJUKUAa1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "#Parameters\n",
        "factor = 6\n",
        "end_lr = lr_max\n",
        "iter = 0\n",
        "total_logs = []\n",
        "\n",
        "#Loss function and optmixation function\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.)\n",
        "step_size = 4*len(train_loader)\n",
        "clr = cyclical_lr(step_size, min_lr=end_lr/factor, max_lr=end_lr)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n",
        "\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_wn-qnsAa1P",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Time to train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wX5cTaKHAa1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "import numpy as np\n",
        "n_epochs = 30\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss (comapre the values of the output model to the actual labels)\n",
        "        loss = criterion(output, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step() # > Where the magic happens\n",
        "        lr_sched_test = scheduler.get_last_lr()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*images.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        # print the decremnet in the validation\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tLearning rate: {}'.format(\n",
        "            epoch, train_loss, valid_loss, lr_sched_test))\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min, \n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_AML_classifier.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "    if epoch % 10 == 0:    \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tLearning rate: {}'.format(\n",
        "            epoch, train_loss, valid_loss, lr_sched_test))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Pug09dlUAa1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw train vs valid loss\n",
        "'''%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss, label=\"Trainig Loss\")\n",
        "plt.plot(valid_loss, label=\"Validation Loss\")\n",
        "plt.legend(frameon=False)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eb193RZCAa1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "2dr66ij9Aa1Z",
        "colab_type": "text"
      },
      "source": [
        "### Notes:\n",
        "\n",
        "- less lr better training\n",
        "\n",
        "- make sure my data is wel distrbuted\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Xt6N-F-5Aa1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}